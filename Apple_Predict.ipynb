{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chienlovecode/Apple_stocks_predict/blob/main/Apple_Predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Cài đặt thư viện\n",
        "import pandas as pd # Import pandas and give it alias pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, col, avg, lag, when\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# #2. Khởi tạo SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockLSTM_PySpark_AAPL\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "nYU-eha1aeCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download AAPL data via yfinance and load into Spark"
      ],
      "metadata": {
        "id": "lwqlKmxsa0cC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAJ_vBxjIsjw"
      },
      "outputs": [],
      "source": [
        "# Bước 1: Tạo SparkSession\n",
        "START = \"2015-01-01\"\n",
        "TODAY = pd.to_datetime(\"today\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Download into pandas\n",
        "pdf = yf.download('AAPL', START, TODAY).reset_index()\n",
        "\n",
        "# Check if the pandas DataFrame is empty\n",
        "if pdf.empty:\n",
        "    raise ValueError(\"The downloaded data is empty. Check your date range or internet connection.\")\n",
        "\n",
        "# Flatten MultiIndex columns (nếu có)\n",
        "if isinstance(pdf.columns, pd.MultiIndex):\n",
        "    # Lấy level 0 (Open, High, Low, Close, Adj Close, Volume, Date)\n",
        "    pdf.columns = pdf.columns.get_level_values(0)\n",
        "\n",
        "# Tạo Spark DataFrame từ Pandas\n",
        "df_spark = spark.createDataFrame(pdf)\n",
        "\n",
        "# Set legacy time parser policy\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") # This line is added to set the legacy time parser\n",
        "\n",
        "# Chuyển cột 'Date' về kiểu ngày và sắp xếp\n",
        "from pyspark.sql.functions import to_date, col\n",
        "df_spark = (\n",
        "    df_spark\n",
        "    .withColumn(\"Date\", to_date(col(\"Date\").cast(\"string\"), \"yyyy-MM-dd\"))\n",
        "    .orderBy(\"Date\")\n",
        ")\n",
        "\n",
        "df_spark.printSchema()\n",
        "df_spark.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering in Spark"
      ],
      "metadata": {
        "id": "YBPXcPfs81sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving Average (MA20)\n",
        "w = Window().orderBy(\"Date\").rowsBetween(-19, 0)\n",
        "df_spark = df_spark.withColumn(\"MA20\", avg(\"Close\").over(w))\n",
        "\n",
        "# RSI calculation\n",
        "df_spark = df_spark.withColumn(\"delta\", col(\"Close\") - lag(\"Close\",1).over(Window.orderBy(\"Date\")))\n",
        "df_spark = df_spark.withColumn(\"gain\", when(col(\"delta\")>0, col(\"delta\")).otherwise(0))\n",
        "df_spark = df_spark.withColumn(\"loss\", when(col(\"delta\")<0, -col(\"delta\")).otherwise(0))\n",
        "\n",
        "w14 = Window().orderBy(\"Date\").rowsBetween(-13, 0)\n",
        "df_spark = df_spark.withColumn(\"avg_gain\", avg(\"gain\").over(w14)) \\\n",
        "                   .withColumn(\"avg_loss\", avg(\"loss\").over(w14))\n",
        "df_spark = df_spark.withColumn(\"RS\", col(\"avg_gain\")/col(\"avg_loss\")) \\\n",
        "                   .withColumn(\"RSI\", 100 - (100/(1+col(\"RS\"))))\n",
        "\n",
        "df_spark.select(\"Date\", \"Close\", \"MA20\", \"RSI\").show(5)"
      ],
      "metadata": {
        "id": "rt_yg0bYcKJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling features with Spark ML"
      ],
      "metadata": {
        "id": "FF6EvPQc9K56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# Check for nulls in relevant columns\n",
        "for column in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"MA20\",\"RSI\"]:\n",
        "    null_count = df_spark.select(count(when(isnan(column) | col(column).isNull(), column))).first()[0]\n",
        "    print(f\"Number of nulls in column {column}: {null_count}\")\n",
        "\n",
        "# Drop rows with nulls in any of the relevant columns\n",
        "df_spark = df_spark.dropna(subset=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"MA20\",\"RSI\"])\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"MA20\",\"RSI\"],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "df_vec = assembler.transform(df_spark)\n",
        "\n",
        "scaler = MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\")\n",
        "scaler_model = scaler.fit(df_vec)\n",
        "df_scaled = scaler_model.transform(df_vec)\n",
        "\n",
        "df_scaled.select(\"Date\", \"features_scaled\").show(5)\n"
      ],
      "metadata": {
        "id": "OxFjUTdAd9Qw",
        "outputId": "5cae617d-ceae-49f3-a79e-106ee38e278f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lỗi tải dữ liệu: Empty dataset received.. Thử lại sau 30 giây... (1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lỗi tải dữ liệu: Empty dataset received.. Thử lại sau 30 giây... (2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lỗi tải dữ liệu: Empty dataset received.. Thử lại sau 30 giây... (3/3)\n",
            "Thất bại sau nhiều lần thử. Vui lòng thử lại sau.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xoá các cột 'Date' và 'Adj Close'\n",
        "df_spark = df_spark.drop('Date', 'Adj Close')\n",
        "\n",
        "# Lấy dữ liệu cột 'Close' về pandas để vẽ\n",
        "df_plot = df_spark.select(\"Close\").toPandas()\n",
        "\n",
        "# Vẽ biểu đồ bằng matplotlib\n",
        "plt.title(\"Close Price Visualization\")\n",
        "plt.plot(df_plot['Close'])\n",
        "plt.xlabel(\"Time (Index)\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KybluF0Eci3o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8b80a709-3fb3-48bc-bac4-afa22c76bb9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_spark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-43f3217b2bd9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Xoá các cột 'Date' và 'Adj Close'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Adj Close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Lấy dữ liệu cột 'Close' về pandas để vẽ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Close\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_spark' is not defined"
          ]
        }
      ]
    }
  ]
}