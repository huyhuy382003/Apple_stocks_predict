{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chienlovecode/Apple_stocks_predict/blob/main/Apple_Predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fAJ_vBxjIsjw"
      },
      "outputs": [],
      "source": [
        "#1. Cài đặt thư viện\n",
        "import pandas as pd # Import pandas and give it alias pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import to_date, col, avg, lag, when\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "# #2. Khởi tạo SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockLSTM_PySpark_AAPL\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Download AAPL data via yfinance and load into Spark\n",
        "START = \"2015-01-01\"\n",
        "TODAY = pd.to_datetime(\"today\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Download into pandas\n",
        "pdf = yf.download('AAPL', START, TODAY).reset_index()\n",
        "\n",
        "# Check if the pandas DataFrame is empty\n",
        "if pdf.empty:\n",
        "    raise ValueError(\"The downloaded data is empty. Check your date range or internet connection.\")\n",
        "\n",
        "# Flatten MultiIndex columns (nếu có)\n",
        "if isinstance(pdf.columns, pd.MultiIndex):\n",
        "    # Lấy level 0 (Open, High, Low, Close, Adj Close, Volume, Date)\n",
        "    pdf.columns = pdf.columns.get_level_values(0)\n",
        "\n",
        "# Tạo Spark DataFrame từ Pandas\n",
        "df_spark = spark.createDataFrame(pdf)\n",
        "\n",
        "# Set legacy time parser policy\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") # This line is added to set the legacy time parser\n",
        "\n",
        "# Chuyển cột 'Date' về kiểu ngày và sắp xếp\n",
        "from pyspark.sql.functions import to_date, col\n",
        "df_spark = (\n",
        "    df_spark\n",
        "    .withColumn(\"Date\", to_date(col(\"Date\").cast(\"string\"), \"yyyy-MM-dd\"))\n",
        "    .orderBy(\"Date\")\n",
        ")\n",
        "\n",
        "df_spark.printSchema()\n",
        "df_spark.show(5)\n"
      ],
      "metadata": {
        "id": "98nTvYgcC3VO",
        "outputId": "1948ab6c-0f77-413b-d613-256005ad46d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Close: double (nullable = true)\n",
            " |-- High: double (nullable = true)\n",
            " |-- Low: double (nullable = true)\n",
            " |-- Open: double (nullable = true)\n",
            " |-- Volume: long (nullable = true)\n",
            "\n",
            "+----------+------------------+------------------+------------------+------------------+---------+\n",
            "|      Date|             Close|              High|               Low|              Open|   Volume|\n",
            "+----------+------------------+------------------+------------------+------------------+---------+\n",
            "|2015-01-02|24.288583755493164| 24.75733822078495|23.848709278537115|24.746229620306494|212818400|\n",
            "|2015-01-05|23.604337692260742|24.137518365002265| 23.41772538901526|24.057541179344305|257142000|\n",
            "|2015-01-06| 23.60655403137207|23.866478974309715|23.244434724352086| 23.66875811738099|263188400|\n",
            "|2015-01-07|23.937572479248047|24.037543101651647| 23.70430543372708|  23.8153846563686|160423600|\n",
            "|2015-01-08| 24.85731315612793|24.915074837750936|24.148627035050126| 24.26637245776407|237458000|\n",
            "+----------+------------------+------------------+------------------+------------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Feature Engineering in Spark\n",
        "# Moving Average (MA20)\n",
        "w = Window().orderBy(\"Date\").rowsBetween(-19, 0)\n",
        "df_spark = df_spark.withColumn(\"MA20\", avg(\"Close\").over(w))\n",
        "\n",
        "# RSI calculation\n",
        "df_spark = df_spark.withColumn(\"delta\", col(\"Close\") - lag(\"Close\",1).over(Window.orderBy(\"Date\")))\n",
        "df_spark = df_spark.withColumn(\"gain\", when(col(\"delta\")>0, col(\"delta\")).otherwise(0))\n",
        "df_spark = df_spark.withColumn(\"loss\", when(col(\"delta\")<0, -col(\"delta\")).otherwise(0))\n",
        "\n",
        "w14 = Window().orderBy(\"Date\").rowsBetween(-13, 0)\n",
        "df_spark = df_spark.withColumn(\"avg_gain\", avg(\"gain\").over(w14)) \\\n",
        "                   .withColumn(\"avg_loss\", avg(\"loss\").over(w14))\n",
        "df_spark = df_spark.withColumn(\"RS\", col(\"avg_gain\")/col(\"avg_loss\")) \\\n",
        "                   .withColumn(\"RSI\", 100 - (100/(1+col(\"RS\"))))\n",
        "\n",
        "df_spark.select(\"Date\", \"Close\", \"MA20\", \"RSI\").show(5)\n"
      ],
      "metadata": {
        "id": "KZaKITzdPFfq",
        "outputId": "1b743ce4-b76a-4f99-f786-3e16ed81d047",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+------------------+------------------+\n",
            "|      Date|             Close|              MA20|               RSI|\n",
            "+----------+------------------+------------------+------------------+\n",
            "|2015-01-02|24.288583755493164|24.288583755493164|              NULL|\n",
            "|2015-01-05|23.604337692260742|23.946460723876953|               0.0|\n",
            "|2015-01-06| 23.60655403137207|23.833158493041992|0.3228638748110484|\n",
            "|2015-01-07|23.937572479248047|23.859261989593506| 32.75096400245195|\n",
            "|2015-01-08| 24.85731315612793| 24.05887222290039| 64.67899754052037|\n",
            "+----------+------------------+------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Scaling features with Spark ML\n",
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "\n",
        "# Check for nulls in relevant columns\n",
        "for column in [\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"MA20\",\"RSI\"]:\n",
        "    null_count = df_spark.select(count(when(isnan(column) | col(column).isNull(), column))).first()[0]\n",
        "    print(f\"Number of nulls in column {column}: {null_count}\")\n",
        "\n",
        "# Drop rows with nulls in any of the relevant columns\n",
        "df_spark = df_spark.dropna(subset=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"MA20\",\"RSI\"])\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"MA20\",\"RSI\"],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "df_vec = assembler.transform(df_spark)\n",
        "\n",
        "scaler = MinMaxScaler(inputCol=\"features_raw\", outputCol=\"features_scaled\")\n",
        "scaler_model = scaler.fit(df_vec)\n",
        "df_scaled = scaler_model.transform(df_vec)\n",
        "\n",
        "df_scaled.select(\"Date\", \"features_scaled\").show(5)"
      ],
      "metadata": {
        "id": "bbS9o261R3dz",
        "outputId": "d696f191-2b40-4859-f409-e77de26483d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nulls in column Open: 0\n",
            "Number of nulls in column High: 0\n",
            "Number of nulls in column Low: 0\n",
            "Number of nulls in column Close: 0\n",
            "Number of nulls in column Volume: 0\n",
            "Number of nulls in column MA20: 0\n",
            "Number of nulls in column RSI: 0\n",
            "+----------+--------------------+\n",
            "|      Date|     features_scaled|\n",
            "+----------+--------------------+\n",
            "|2015-01-05|[0.01471649271360...|\n",
            "|2015-01-06|[0.01307605044738...|\n",
            "|2015-01-07|[0.01369473062004...|\n",
            "|2015-01-08|[0.01559764132614...|\n",
            "|2015-01-09|[0.01882219492671...|\n",
            "|2015-01-12|[0.01875660525088...|\n",
            "|2015-01-13|[0.01765983035581...|\n",
            "|2015-01-14|[0.01541949961150...|\n",
            "|2015-01-15|[0.01631938376838...|\n",
            "|2015-01-16|[0.01353539583343...|\n",
            "|2015-01-20|[0.01429464712023...|\n",
            "|2015-01-21|[0.01533513753738...|\n",
            "|2015-01-22|[0.01656312361042...|\n",
            "|2015-01-23|[0.01847535327772...|\n",
            "|2015-01-26|[0.01982520650426...|\n",
            "|2015-01-27|[0.01858783748185...|\n",
            "|2015-01-28|[0.02347162329273...|\n",
            "|2015-01-29|[0.02224361826114...|\n",
            "|2015-01-30|[0.02419340728778...|\n",
            "|2015-02-02|[0.02386531305175...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}